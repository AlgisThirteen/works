{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Crane Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Structure:\n",
    "1. Code needed to perform the analysis (can be skipped)\n",
    "2. Findings Presentation\n",
    "3. Addendum with full outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook notation:\n",
    "- '# main header for the notebook:\n",
    "- '## #.                     Topic header\n",
    "- '### #.#                   Section header\n",
    "- '#### #.#.#                Subsection Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section connects and fetches the data for the next stages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting so that all rows are dispalyed for analysis\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlining the connection string\n",
    "Driver = '{SQL Server}'\n",
    "server = '' \n",
    "database = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials\n",
    "uid = ''\n",
    "psswd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the connection to the server\n",
    "while True:\n",
    "    try:\n",
    "        conn = pyodbc.connect(f'Driver={Driver};Server={server};Database={database};uid={uid};pwd={psswd}')\n",
    "    except pyodbc.OperationalError:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Datasets\n",
    "sql = 'SELECT * FROM database.table'\n",
    "analysed_data = pd.read_sql(sql, conn)\n",
    "sql1 = 'SELECT * FROM database.table'\n",
    "attom_data = pd.read_sql(sql1, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Running analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is used to run the analysis and can be ignored by most consumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Match on ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match up the Id's\n",
    "attom_id = pd.DataFrame(attom_data['ATTOMID'])\n",
    "attom_id.columns = ['attomid']\n",
    "pc_id = pd.DataFrame(analysed_data['attomid'].unique())\n",
    "pc_id.columns = ['attomid']\n",
    "merge_id = pd.merge(attom_id, pc_id, how='inner', on='attomid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distinct attomid with state from paper crane data\n",
    "distinct_states = analysed_data[['attomid', 'situsstate']].drop_duplicates()\n",
    "\n",
    "# get counts by value\n",
    "state_at = attom_data['SitusState'].value_counts()\n",
    "state_pc = distinct_states['situsstate'].value_counts()\n",
    "\n",
    "# transform to dataframe\n",
    "state_at = pd.DataFrame([(key, item) for key, item in state_at.items()])\n",
    "state_at.columns = ['state', 'attomCount']\n",
    "state_pc = pd.DataFrame([(key, item) for key, item in state_pc.items()])\n",
    "state_pc.columns = ['state', 'paperCraneCount']\n",
    "\n",
    "# combine the above datafreames into one\n",
    "state_merge = pd.merge(state_at, state_pc, how='outer', on='state')\n",
    "state_merge['matchRate'] = state_merge['paperCraneCount'] / state_merge['attomCount']\n",
    "sorted_StateMatch = state_merge.sort_values(by=['matchRate'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to show match on ID visual\n",
    "matched = len(merge_id) / len(attom_id)\n",
    "matchLabels = ['Matched', 'Unmatched']\n",
    "datapoints = [matched, 1-matched]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Categorical and Continous Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataset to change the date format column type from object to datetime\n",
    "mask = analysed_data.astype(str).apply(lambda x : x.str.match(r'(\\d{2,4}-\\d{2}-\\d{2,4})+').all())\n",
    "analysed_data.loc[:,mask] = analysed_data.loc[:,mask].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical, Continuous and Date column lists\n",
    "cat_columns = [column for column in analysed_data.columns if analysed_data[column].dtype == 'object']\n",
    "cont_columns = [column for column in analysed_data.columns if analysed_data[column].dtype != 'object']\n",
    "date_columns = [column for column in analysed_data.columns if str(analysed_data[column].dtype)[:4] == 'date']\n",
    "\n",
    "# move the bool columns from the cont to cat list\n",
    "bool_columns = [column for column in analysed_data.columns if analysed_data[column].dtype == 'bool']\n",
    "cat_columns = cat_columns + bool_columns\n",
    "cont_columns = list(set(cont_columns) - set(bool_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_formats = ['nn/nn/nnnn', 'nnnn/nn/nn', 'nn/nn/nn', 'n/n/nn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the Object columns and checking for date formats in the list \n",
    "# Only take the first 100 examples of the column for format\n",
    "# Creating an array and looping through object columns\n",
    "matched_formats = {}\n",
    "for column in cat_columns:\n",
    "    test_array = np.array(analysed_data[column][:100])\n",
    "    format_list = []\n",
    "    for cell in test_array:\n",
    "        if cell != None:\n",
    "            cell_format = ''\n",
    "            for character in str(cell):\n",
    "                if character.isnumeric() == True:\n",
    "                    cell_format += 'n'\n",
    "                elif character.isalpha() == True:\n",
    "                    cell_format += 'l'\n",
    "                else:\n",
    "                    cell_format += '/'\n",
    "        if cell_format not in format_list:\n",
    "            format_list.append(cell_format)\n",
    "    test_array = []\n",
    "    if len(format_list) == 1:\n",
    "        for dformat in date_formats:\n",
    "            if dformat == format_list[0][:len(dformat)] and column not in matched_formats:                    \n",
    "                matched_formats[column] = dformat\n",
    "            elif dformat == format_list[0][:len(dformat)] and column in matched_formats:\n",
    "                if len(dformat) > len(matched_formats[column]):\n",
    "                    matched_formats[column] = dformat\n",
    "            else:\n",
    "                pass\n",
    "date_columns = date_columns + list(matched_formats.keys())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove date_columns list from cat_columns\n",
    "cat_columns = list(set(cat_columns) - set(date_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column types for the dataset\n",
    "column_list = analysed_data.dtypes\n",
    "dataset_columns = pd.DataFrame([(key, item) for key, item in column_list.items()])\n",
    "dataset_columns.columns = ['ColumnName', 'ColumnType']\n",
    "\n",
    "# Generate the count of instance by column type\n",
    "colType_count = dataset_columns['ColumnType'].value_counts()\n",
    "colType = np.array([(str(key)) for key, item in colType_count.items()])\n",
    "colCount = np.array([(item) for key, item in colType_count.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3. Basic Stats for Categorical and Continous Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats for categorical columns\n",
    "cat_stats = analysed_data[cat_columns].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset stats for continuous columns\n",
    "cont_stats = analysed_data[cont_columns].describe().transpose().apply(lambda s: s.apply('{0:.5f}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 NULL, 0, Distinct count by Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of istances of a value\n",
    "null_count = analysed_data.isna().sum()\n",
    "zero_count = (analysed_data == 0).astype(int).sum(axis=0)\n",
    "\n",
    "# convert into a dataframe\n",
    "null_count_df = pd.DataFrame([(key, item) for key, item in null_count.items()])\n",
    "null_count_df.columns = ['ColumnName', 'nullCount']\n",
    "\n",
    "zero_count_df = pd.DataFrame([(key, item) for key, item in zero_count.items()])\n",
    "zero_count_df.columns = ['ColumnName', 'zeroCount']\n",
    "\n",
    "full_count = pd.merge(null_count_df, zero_count_df, how='outer', on='ColumnName')\n",
    "\n",
    "# counting up unique values in each column\n",
    "unique_val = {}\n",
    "for col in analysed_data:\n",
    "    unique_val[col] = analysed_data[col].unique()\n",
    "\n",
    "distinct_values = []\n",
    "for key, item in unique_val.items():\n",
    "    distinct_values.append([key, len(item)])\n",
    "distinct_values = pd.DataFrame(distinct_values)\n",
    "distinct_values.columns = ['ColumnName','DistinctCount']\n",
    "\n",
    "full_count = pd.merge(full_count, distinct_values, how='outer', on='ColumnName').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of zeros and NULLS are there in the entire dataset?\n",
    "nulls = full_count['nullCount'].sum()\n",
    "zeros = full_count['zeroCount'].sum()\n",
    "\n",
    "# How many cells are there in the dataset in total\n",
    "all_cells = analysed_data.shape[0] * analysed_data.shape[1]\n",
    "\n",
    "# The values for pie chart that will be displayed in secion 2\n",
    "labels = 'Filled Cells', 'Zeros', 'NULLs'\n",
    "sizes = [all_cells - zeros - nulls, zeros, nulls]\n",
    "explode = [0.2, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 Correlation Calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the correlation of the dataset\n",
    "corr_data = analysed_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstacked Correlation analysis\n",
    "corr_data_us = corr_data.unstack()\n",
    "sorted_corr = corr_data_us.sort_values(ascending=False)\n",
    "\n",
    "# move the unstacked data into a list\n",
    "corr_list = [(key[0], key[1], item) for key, item in sorted_corr.items()]\n",
    "\n",
    "# remove the correlation with thyself\n",
    "inx = 0\n",
    "test_list = []\n",
    "while inx < len(corr_list):\n",
    "    if corr_list[inx][0] == corr_list[inx][1]:\n",
    "        test_list.append(corr_list[inx])\n",
    "        corr_list.remove(corr_list[inx])\n",
    "        inx -= 1\n",
    "    inx += 1\n",
    "    \n",
    "corr_list_df = pd.DataFrame(corr_list)\n",
    "corr_list_df.columns = ['ColumnName1', 'ColumnName2', 'Correlation']\n",
    "corr_list_df = corr_list_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.3.6 String Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block creates a DataFrame with the value lenght\n",
    "value_length = {}\n",
    "for name in dataset_columns[dataset_columns['ColumnType'] != 'bool']['ColumnName']:\n",
    "    value_length[name] = [len(str(var)) for var in analysed_data[name]]\n",
    "value_lengthDF = pd.DataFrame(value_length)\n",
    "\n",
    "del value_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block creates DataFrame with average, min and max lenght of string for each column\n",
    "columnLenght_info = {}\n",
    "for name in value_lengthDF.columns:\n",
    "    columnLenght_info[name] = [np.average(value_lengthDF[name]),np.amin(value_lengthDF[name]),\n",
    "                               np.amax(value_lengthDF[name])]\n",
    "columnLenghtDF = pd.DataFrame(columnLenght_info).transpose()\n",
    "columnLenghtDF.columns = ['average', 'minimum', 'maximum']\n",
    "\n",
    "del columnLenght_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.7 Dataset characteristics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block creates DataFrame for describe function for continuous and categorical columns\n",
    "contDesc = analysed_data[cont_columns].describe().apply(lambda s: s.apply('{0:.1f}'.format)).transpose()\n",
    "catDesc = analysed_data[cat_columns].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with some negative values\n",
    "someNegColumns = contDesc.loc[contDesc['min'] < '0'][['mean', 'min', 'max']]\n",
    "\n",
    "# Columns with all negative values\n",
    "allNegColumns = contDesc.loc[contDesc['max'] < '0'][['mean', 'min', 'max']]\n",
    "\n",
    "# Combining the two DataFrames\n",
    "colWithNegValues = pd.concat([someNegColumns, allNegColumns], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block creates a DataFrame with the column Name and % of negative values in it\n",
    "negValCount = {}\n",
    "for name in cont_columns:\n",
    "    instance = 0\n",
    "    for val in analysed_data[name]:\n",
    "        if val < 0:\n",
    "            instance += 1\n",
    "    if instance > 0:\n",
    "        negValCount[name] = instance / analysed_data.shape[0] * 100\n",
    "\n",
    "# creates lists for 2 columns \n",
    "negValKey, negValItem = [], []\n",
    "for key, item in negValCount.items():\n",
    "    negValKey.append(key)\n",
    "    negValItem.append(item)\n",
    "\n",
    "# building the DataFrame to for Negative Value % of total\n",
    "negValCountDF = pd.DataFrame({'ColumnName': negValKey, 'Neg Val %': negValItem})\n",
    "negValCountDF = negValCountDF.sort_values(by=['Neg Val %'], ascending=False)\n",
    "    \n",
    "# Removing the lists\n",
    "del negValKey\n",
    "del negValItem    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.8 Date Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame for the date columns\n",
    "if len(date_columns) > 0:\n",
    "    dateDescribe = analysed_data[date_columns].describe().transpose()\n",
    "else:\n",
    "    dateDescribe = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Findings Presentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section contains minimal amount of code and is used to present the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 Basic dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Dataset Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "print(f'The dataset has {analysed_data.shape[0]:,} rows and {analysed_data.shape[1]:,} and columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Columns by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(cat_columns)} categorical columns and {len(cont_columns)} numerical columns in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Set Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Match Rate is {matched:.2%}')\n",
    "plt.pie(x=datapoints, labels=matchLabels, autopct='%1.2f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dataset Match by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the match rates by state\n",
    "plt.bar(sorted_StateMatch['state'], sorted_StateMatch['matchRate'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Columns by Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(colType, colCount)\n",
    "plt.xlabel('Column Category')\n",
    "plt.ylabel('Instance count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{colCount[0] / analysed_data.shape[1]:.2%} of all columns are numerical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.5 NULL and 0 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {nulls:,} cells with NULL value and {zeros:,} cells with 0 value')\n",
    "print(f'A {nulls/all_cells:,.2%} of cells have NULL value and {zeros/all_cells:,.2%} of cells have 0 value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, explode=explode, autopct='%1.1f%%')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Column Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1 Negative Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negValCountDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.barplot(x=negValCountDF['Neg Val %'], y=negValCountDF['ColumnName'], data=negValCountDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 Date Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date columns in the dataset with column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(date_columns) > 0:\n",
    "    analysed_data[date_columns].describe() \n",
    "else:\n",
    "    print('\\n No date columns were detected in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date Ranges in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(date_columns) > 0:\n",
    "    dateDescribe\n",
    "else:\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Addendum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Top 10 Rows of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dataset Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.2.2 Column Lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column lenghts are sorted largest to smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnLenghtDF.sort_values(by=['maximum'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Columns with NULL, Zero and distinct counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Correlation Matrix for Continous Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,10))\n",
    "ax = sns.heatmap(corr_data, vmin=-1, vmax=1, center=0,\n",
    "                 cmap=sns.diverging_palette(20,220,n=200),\n",
    "                 square=True\n",
    "                )\n",
    "ax.set_xticklabels(ax.get_xticklabels(),\n",
    "                  rotation=45,\n",
    "                  horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Highest correlated fields (positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "highest_positive_corr = corr_list_df.loc[corr_list_df['Correlation'] > correlation_threshold]\n",
    "highest_positive_corr.sort_values(by = ['Correlation'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.4.3 Highest correlated fields (negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_negative_corr = corr_list_df.loc[corr_list_df['Correlation'] < -correlation_threshold]\n",
    "highest_negative_corr.sort_values(by = ['Correlation'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.5 Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Histograms for Continous Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms for all continous columns. The graphs are aranged in alphabetical order. Below the graph is a list of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col_grid = 5\n",
    "row_grid = (len(cont_columns) // col_grid) + 1\n",
    "fig = analysed_data[cont_columns].hist(bins=50, figsize=(15,40), layout=(row_grid, col_grid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, col in enumerate(cont_columns):\n",
    "    print(f'{idx + 1} {col}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
